{
    "internal": [
        [
            "<|BEGIN-VISIBLE-CHAT|>",
            "How can I help you today?"
        ],
        [
            "Hello how are you functioning?  I'm going to give you a summary of the project we have been working on and I'll give you the code in the next message.\n\n### Summary of Lucid_Autonomy\n\n#### Overview\n`Lucid_Autonomy` is an extension designed to enable a Large Language Model (LLM) to interact autonomously with a computer. It leverages various models and tools to detect objects on the screen, process images, and perform actions such as moving the mouse, clicking, typing text, and pressing keys.\n\n#### Key Components\n1. **Models**:\n   - **Owlv2 Model**: Used for object detection based on text queries.\n   - **MiniCPM-Llama3-V-2_5 Model**: Used for processing images and generating descriptions.\n\n2. **Tools**:\n   - **Gradio Interface**: Provides a user interface for configuring and triggering actions.\n   - **PyAutoGUI**: Used for automating mouse and keyboard actions.\n   - **Screeninfo**: Used for getting information about connected monitors.\n\n3. **Functions**:\n   - **take_screenshot**: Captures a screenshot of a specified monitor.\n   - **query_image**: Processes the screenshot using the Owlv2 model to detect objects.\n   - **crop_images**: Crops out detected objects and processes them with the MiniCPM model.\n   - **process_with_vision_model**: Processes images with the MiniCPM model and returns descriptions.\n   - **execute_tasks**: Executes tasks listed in the LLM's output, such as moving the mouse, clicking, typing text, and pressing keys.\n   - **execute_oob_tasks**: Executes out-of-band (OOB) tasks after a delay.\n\n#### Ultimate Intent\nThe ultimate intent of `Lucid_Autonomy` is to provide the LLM with \"eyes\" and \"hands\" to interact autonomously with a computer. This includes the ability to:\n- Take screenshots and detect objects on the screen.\n- Move the mouse and click on specific coordinates or detected objects.\n- Type text and press keys.\n- Perform complex tasks such as navigating a web browser or interacting with a user interface.\n\n#### How It Fits into Textgen\n`Lucid_Autonomy` is designed to work within the `textgen` ecosystem, which is a powerful web-based interface for running large language models (LLMs) locally. The extension enhances the capabilities of `textgen` by allowing the LLM to interact with the user's computer, effectively giving it the ability to perform tasks that would otherwise require human intervention.\n\n### Conclusion\n`Lucid_Autonomy` is a powerful extension that enhances the capabilities of LLMs by enabling them to interact autonomously with a computer. It fits seamlessly into the `textgen` ecosystem, providing a robust framework for automating tasks and enhancing user interaction. When starting a new conversation, providing clear context, relevant code snippets, goals, and constraints will help ensure effective and efficient assistance.",
            "Thank you for the detailed summary of the `Lucid_Autonomy` project. It sounds like a sophisticated and innovative extension that significantly enhances the capabilities of Large Language Models. I'm ready to review the code and provide any assistance you need. Please go ahead and share the code, and let me know if there are any specific areas you'd like me to focus on or any particular issues you're encountering."
        ],
        [
            "Here is the code, I want you to write notes and observations; contextualize the code, and I'll tell you what I want you to do next:\n\nimport gradio as gr\nimport json\nimport pyautogui\nimport screeninfo\nfrom transformers import Owlv2Processor, Owlv2ForObjectDetection, AutoModel, AutoTokenizer\nfrom PIL import Image, ImageDraw\nimport torch\nimport os\nimport shutil\nimport re  # Import the re module for regular expression operations\nimport json\nimport time\nimport threading\n\noob_tasks_file = \"extensions/Lucid_Autonomy/oob_tasks.json\"\n\n# Configurable variables\nMONITOR_INDEX = 1  # Index of the monitor to capture (0 for the first monitor, 1 for the second monitor, etc.)\nTEXT_QUERIES = \"colored hyperlink text,clickable icons,text bar field,clickable UI buttons,UI tabs,Interactive UI Elements,blue text hyperlink\"  # Comma-separated text queries for object detection\nSCORE_THRESHOLD = 0.24  # Score threshold for filtering low-probability predictions\nVISION_MODEL_QUESTION = \"Keep your response to less than 2 sentences: What is the text in this image or what does this image describe? The AI is aiding in helping those with visual handicaps so the AI will focus on providing all of the relevant information for the listener.\"\nFULL_IMAGE_VISION_MODEL_QUESTION = \"This is a computer screenshot, identify all article text if the screen is displaying an article.  Describe what you see in this image, if you see UI elements describe them and their general location and if you see text read the text verbatim.  You are aiding in helping those with visual handicaps so you need to try and focus on providing all of the relevant text on a screen and contextualizing it for the listener.\"\n\n# Use GPU if available\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\n# Global variables for models\nowlv2_model = None\nowlv2_processor = None\nminicpm_llama_model = None\nminicpm_llama_tokenizer = None\n\nVISION_MODEL_ID = \"/home/myself/Desktop/miniCPM_llava3V/MiniCPM-V-2_6\"\n\n#google/owlv2-base-patch16-ensemble\n#google/owlv2-base-patch16\n#google/owlv2-base-patch16-finetuned\n\ndef load_owlv2_model():\n    global owlv2_model, owlv2_processor\n    owlv2_model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\", trust_remote_code=True).to(device)\n    owlv2_processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\", trust_remote_code=True)\n    print(\"Owlv2 model loaded.\")\n\ndef unload_owlv2_model():\n    global owlv2_model, owlv2_processor\n    if owlv2_model is not None:\n        del owlv2_model\n        del owlv2_processor\n        torch.cuda.empty_cache()\n        print(\"Owlv2 model unloaded.\")\n\ndef load_minicpm_llama_model():\n    global minicpm_llama_model, minicpm_llama_tokenizer\n    if \"int4\" in VISION_MODEL_ID:\n        # Load the 4-bit quantized model and tokenizer\n        minicpm_llama_model = AutoModel.from_pretrained(\n            VISION_MODEL_ID,\n            device_map={\"\": device},\n            trust_remote_code=True,\n            torch_dtype=torch.float16  # Use float16 as per the example code for 4-bit models\n        ).eval()\n        minicpm_llama_tokenizer = AutoTokenizer.from_pretrained(\n            VISION_MODEL_ID,\n            trust_remote_code=True\n        )\n        print(\"MiniCPM-Llama3 4-bit quantized model loaded on-demand.\")\n    else:\n        # Load the standard model and tokenizer\n        minicpm_llama_model = AutoModel.from_pretrained(\n            VISION_MODEL_ID,\n            device_map={\"\": device},  # Use the specified CUDA device\n            trust_remote_code=True,\n            torch_dtype=torch.bfloat16\n        ).eval()\n        minicpm_llama_tokenizer = AutoTokenizer.from_pretrained(\n            VISION_MODEL_ID,\n            trust_remote_code=True\n        )\n        print(\"MiniCPM-Llama3 standard model loaded on-demand.\")\n\ndef unload_minicpm_llama_model():\n    global minicpm_llama_model, minicpm_llama_tokenizer\n    if minicpm_llama_model is not None:\n        del minicpm_llama_model\n        del minicpm_llama_tokenizer\n        torch.cuda.empty_cache()\n        print(\"MiniCPM-Llama3 model unloaded.\")\n\ndef take_screenshot(monitor_index, text_queries, score_threshold, vision_model_question, full_image_vision_model_question, output_filename=\"screenshot.png\"):\n    \"\"\"\n    Takes a screenshot of a specific monitor in a multi-monitor setup.\n\n    Args:\n        monitor_index (int): The index of the monitor to capture.\n        text_queries (str): Comma-separated text queries for object detection.\n        score_threshold (float): The score threshold for filtering low-probability predictions.\n        vision_model_question (str): The question to ask the vision model for cropped images.\n        full_image_vision_model_question (str): The question to ask the vision model for the full image.\n        output_filename (str): The filename to save the screenshot as (default is \"screenshot.png\").\n    \"\"\"\n    monitor_index = int(monitor_index)\n    score_threshold = float(score_threshold)\n\n    # Get information about all monitors\n    monitors = screeninfo.get_monitors()\n\n    if monitor_index >= len(monitors):\n        raise ValueError(f\"Monitor index {monitor_index} is out of range. There are only {len(monitors)} monitors.\")\n\n    # Get the region of the specified monitor\n    monitor = monitors[monitor_index]\n    region = (monitor.x, monitor.y, monitor.width, monitor.height)\n\n    # Take the screenshot\n    output_dir = \"extensions/Lucid_Autonomy\"\n    os.makedirs(output_dir, exist_ok=True)\n    screenshot_path = os.path.join(output_dir, output_filename)\n    screenshot = pyautogui.screenshot(region=region)\n    screenshot.save(screenshot_path)\n    print(f\"Screenshot saved as {screenshot_path}\")\n\n    # Process the image using the Owlv2 model\n    annotated_image_path, human_annotated_image_path, json_output_path = query_image(screenshot_path, text_queries, score_threshold)\n\n    # Load the miniCPM-lama3-v-2_5 model\n    load_minicpm_llama_model()\n\n    # Process the entire screenshot with the minicpm model\n    full_image_response = process_with_vision_model(annotated_image_path, full_image_vision_model_question)\n\n    # Unload the miniCPM-lama3-v-2_5 model after processing the full image\n    unload_minicpm_llama_model()\n\n    # Create the output directory if it doesn't exist\n    output_dir = \"extensions/Lucid_Autonomy/ImageOutputTest\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Clear the output directory\n    for filename in os.listdir(output_dir):\n        file_path = os.path.join(output_dir, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n    # Crop the detected objects and save them to the output directory\n    crop_images(annotated_image_path, json_output_path, output_dir, full_image_response)\n\n    print(\"Processing complete. Check the output directory for the cropped images and results.\")\n\ndef query_image(img_path, text_queries, score_threshold):\n    \"\"\"\n    Processes the image using the Owlv2 model to detect objects based on text queries.\n\n    Args:\n        img_path (str): The path to the input image.\n        text_queries (str): Comma-separated text queries for object detection.\n        score_threshold (float): The score threshold for filtering low-probability predictions.\n    \"\"\"\n    load_owlv2_model()\n\n    img = Image.open(img_path)\n    text_queries = text_queries.split(\",\")\n\n    size = max(img.size)\n    target_sizes = torch.Tensor([[size, size]])\n    inputs = owlv2_processor(text=text_queries, images=img, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = owlv2_model(**inputs)\n\n    outputs.logits = outputs.logits.cpu()\n    outputs.pred_boxes = outputs.pred_boxes.cpu()\n    results = owlv2_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes)\n    boxes, scores, labels = results[0][\"boxes\"], results[0][\"scores\"], results[0][\"labels\"]\n\n    result_labels = []\n    img_pil = img.copy()\n    draw = ImageDraw.Draw(img_pil)\n\n    human_img_pil = img.copy()\n    human_draw = ImageDraw.Draw(human_img_pil)\n\n    for box, score, label in zip(boxes, scores, labels):\n        box = [int(i) for i in box.tolist()]\n        if score < score_threshold:\n            continue\n        result_labels.append([box, text_queries[label.item()]])\n        # Do not draw the text label within the bounding box\n        # draw.text((box[0], box[1]), text_queries[label.item()], fill=\"red\")\n\n        # Draw the boxes for the human-annotated image\n        human_draw.rectangle(box, outline=\"red\", width=2)\n\n    # Save the annotated image for debugging\n    annotated_image_path = \"extensions/Lucid_Autonomy/annotated_image.png\"\n    img_pil.save(annotated_image_path)\n\n    # Save the human-annotated image for human inspection\n    human_annotated_image_path = \"extensions/Lucid_Autonomy/Human_annotated_image.png\"\n    human_img_pil.save(human_annotated_image_path)\n\n    # Save the JSON output to a file\n    json_output_path = \"extensions/Lucid_Autonomy/output.json\"\n    with open(json_output_path, 'w') as f:\n        json.dump(result_labels, f)\n\n    unload_owlv2_model()\n\n    return annotated_image_path, human_annotated_image_path, json_output_path\n\ndef crop_images(image_path, json_output_path, output_dir=\"extensions/Lucid_Autonomy/ImageOutputTest\", full_image_response=\"\"):\n    \"\"\"\n    Crops out the detected objects from the input image based on the bounding box coordinates.\n\n    Args:\n        image_path (str): The path to the input image.\n        json_output_path (str): The path to the JSON file containing the bounding box coordinates.\n        output_dir (str): The directory to save the cropped images (default is \"extensions/Lucid_Autonomy/ImageOutputTest\").\n        full_image_response (str): The response from the vision model for the full image.\n    \"\"\"\n    with open(json_output_path, 'r') as f:\n        data = json.load(f)\n\n    img = Image.open(image_path)\n\n    results = []\n\n    # Load the miniCPM-lama3-v-2_5 model once before processing the images\n    load_minicpm_llama_model()\n\n    for i, item in enumerate(data):\n        box = item[0]\n        label = item[1]\n        cropped_img = img.crop(box)\n        cropped_img_path = f\"{output_dir}/{label}_{i}.png\"\n        cropped_img.save(cropped_img_path)\n\n        # Process the cropped image with the miniCPM-lama3-v-2_5 model\n        vision_model_response = process_with_vision_model(cropped_img_path, VISION_MODEL_QUESTION)\n\n        # Store the results in a structured format\n        results.append({\n            \"image_name\": f\"{label}_{i}.png\",\n            \"coordinates\": box,\n            \"description\": vision_model_response,\n            \"children\": []\n        })\n\n    # Unload the miniCPM-lama3-v-2_5 model after processing all images\n    unload_minicpm_llama_model()\n\n    # Determine parent-child relationships\n    for i, result in enumerate(results):\n        for j, other_result in enumerate(results):\n            if i != j:\n                box1 = result[\"coordinates\"]\n                box2 = other_result[\"coordinates\"]\n                center_x1 = (box1[0] + box1[2]) // 2\n                center_y1 = (box1[1] + box1[3]) // 2\n                if box2[0] <= center_x1 <= box2[2] and box2[1] <= center_y1 <= box2[3]:\n                    other_result[\"children\"].append(result[\"image_name\"])\n\n    # Save the results to a JSON file\n    results_json_path = f\"{output_dir}/results.json\"\n    with open(results_json_path, 'w') as f:\n        json.dump(results, f, indent=4)\n\n    # Create the compressed results JSON file\n    compressed_results_json_path = f\"{output_dir}/compressedresults.json\"\n    compressed_results = [{\"image_name\": result[\"image_name\"], \"description\": result[\"description\"], \"children\": result[\"children\"]} for result in results]\n    with open(compressed_results_json_path, 'w') as f:\n        json.dump(compressed_results, f, indent=4)\n\n    # Add the full image response to the results\n    if full_image_response:\n        results.append({\n            \"image_name\": \"full_image.png\",\n            \"coordinates\": [],\n            \"description\": full_image_response,\n            \"children\": []\n        })\n        compressed_results.append({\n            \"image_name\": \"full_image.png\",\n            \"description\": full_image_response,\n            \"children\": []\n        })\n\n        # Save the updated results to the JSON files\n        with open(results_json_path, 'w') as f:\n            json.dump(results, f, indent=4)\n        with open(compressed_results_json_path, 'w') as f:\n            json.dump(compressed_results, f, indent=4)\n\ndef process_with_vision_model(image_path, question):\n    \"\"\"\n    Processes the image with the miniCPM-lama3-v-2_5 model and returns the response.\n\n    Args:\n        image_path (str): The path to the input image.\n        question (str): The question to ask the vision model.\n\n    Returns:\n        str: The response from the vision model.\n    \"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    messages = [\n        {\"role\": \"user\", \"content\": f\"{question}\"}\n    ]\n    # Define the generation arguments\n    generation_args = {\n        \"max_new_tokens\": 1024,\n        \"repetition_penalty\": 1.05,\n        \"num_beams\": 3,\n        \"top_p\": 0.9,\n        \"top_k\": 1,\n        \"temperature\": 0.1,\n        \"sampling\": True,\n    }\n    if \"int4\" in \"openbmb/MiniCPM-Llama3-V-2_5\":\n        # Disable streaming for the 4-bit model\n        generation_args[\"stream\"] = False\n        # Use the model.chat method with streaming enabled\n        vision_model_response = \"\"\n        for new_text in minicpm_llama_model.chat(\n                image=image,\n                msgs=messages,\n                tokenizer=minicpm_llama_tokenizer,\n                **generation_args\n        ):\n            vision_model_response += new_text\n            print(new_text, flush=True, end='')\n\n    else:\n        minicpm_llama_model.to(device)\n        vision_model_response = minicpm_llama_model.chat(\n            image=image,\n            msgs=messages,\n            tokenizer=minicpm_llama_tokenizer,\n            **generation_args\n        )\n    return vision_model_response\n\n# Global variables for Gradio inputs\nglobal_vars = {\n    \"global_monitor_index\": MONITOR_INDEX,\n    \"global_text_queries\": TEXT_QUERIES,\n    \"global_score_threshold\": SCORE_THRESHOLD,\n    \"global_vision_model_question\": VISION_MODEL_QUESTION,\n    \"global_full_image_vision_model_question\": FULL_IMAGE_VISION_MODEL_QUESTION\n}\n\ndef ui():\n    \"\"\"\n    Creates custom gradio elements when the UI is launched.\n    \"\"\"\n    with gr.Blocks() as demo:\n        with gr.Row():\n            with gr.Column():\n                monitor_index = gr.Textbox(label=\"Monitor Index\", value=str(MONITOR_INDEX))\n                text_queries = gr.Textbox(label=\"Text Queries\", value=TEXT_QUERIES)\n                score_threshold = gr.Textbox(label=\"Score Threshold\", value=str(SCORE_THRESHOLD))\n                vision_model_question = gr.Textbox(label=\"Vision Model Question\", value=VISION_MODEL_QUESTION)\n                full_image_vision_model_question = gr.Textbox(label=\"Full Image Vision Model Question\", value=FULL_IMAGE_VISION_MODEL_QUESTION)\n                take_screenshot_button = gr.Button(value=\"Take Screenshot\")\n\n        # Update global variables when the user changes the input fields\n        monitor_index.change(lambda x: global_vars.update({\"global_monitor_index\": int(x)}), inputs=monitor_index, outputs=None)\n        text_queries.change(lambda x: global_vars.update({\"global_text_queries\": x}), inputs=text_queries, outputs=None)\n        score_threshold.change(lambda x: global_vars.update({\"global_score_threshold\": float(x)}), inputs=score_threshold, outputs=None)\n        vision_model_question.change(lambda x: global_vars.update({\"global_vision_model_question\": x}), inputs=vision_model_question, outputs=None)\n        full_image_vision_model_question.change(lambda x: global_vars.update({\"global_full_image_vision_model_question\": x}), inputs=full_image_vision_model_question, outputs=None)\n\n        take_screenshot_button.click(\n            fn=take_screenshot,\n            inputs=[monitor_index, text_queries, score_threshold, vision_model_question, full_image_vision_model_question],\n            outputs=None\n        )\n\n    return demo\n\ndef input_modifier(user_input, state):\n    \"\"\"\n    Modifies the user input before it is processed by the LLM.\n    \"\"\"\n    return user_input\n\ndef output_modifier(output, state):\n    \"\"\"\n    Modifies the LLM output before it is presented in the UI.\n    \"\"\"\n    global oob_tasks\n    oob_tasks = []\n\n    # Search for the \"Autonomy_Tasks:\" trigger phrase in the LLM's output\n    if \"Autonomy_Tasks:\" in output:\n        # Clean the chat history\n        state[\"history\"] = history_modifier(state[\"history\"])\n        # Extract tasks and execute them\n        tasks_section = output.split(\"Autonomy_Tasks:\")[-1].strip()\n        if tasks_section:\n            execute_tasks(tasks_section)\n        # Append new compressed results\n        compressed_results_json_path = \"extensions/Lucid_Autonomy/ImageOutputTest/compressedresults.json\"\n        with open(compressed_results_json_path, 'r') as f:\n            compressed_results = json.load(f)\n        output = append_compressed_results(output, compressed_results)\n\n        # Write OOB tasks to a JSON file\n        with open(oob_tasks_file, 'w') as f:\n            json.dump(oob_tasks, f)\n\n        # Trigger the execution of OOB tasks after a delay\n        threading.Thread(target=execute_oob_tasks_with_delay).start()\n\n    return output\n\noob_tasks = []\n\ndef execute_tasks(tasks_section):\n    \"\"\"\n    Executes the tasks listed in the tasks_section.\n    \"\"\"\n    global oob_tasks\n    oob_tasks = []\n    tasks = tasks_section.split(\"\\n\")\n    for task in tasks:\n        task = task.strip()\n        if task.startswith(\"MouseMover =\"):\n            image_name = task.split(\"=\")[1].strip()\n            move_mouse_to_image(image_name)\n        elif task.startswith(\"MouseClick =\"):\n            value = task.split(\"=\")[1].strip()\n            if \",\" in value:  # Check if the value contains coordinates\n                coordinates = value\n                left_click_coordinates_raw(coordinates)\n            else:  # Otherwise, treat it as an image name\n                image_name = value\n                left_click_image(image_name)\n        elif task.startswith(\"TextInput =\"):\n            text = task.split(\"=\")[1].strip()\n            text_input(text)\n        elif task.startswith(\"SpecialKey =\"):\n            key = task.split(\"=\")[1].strip()\n            special_key(key)\n        elif task.startswith(\"FunctionKey =\"):\n            key = task.split(\"=\")[1].strip()\n            function_key(key)\n        elif task.startswith(\"KeyCombo =\"):\n            combo = task.split(\"=\")[1].strip()\n            key_combo(combo)\n        elif task.startswith(\"Delay =\"):\n            delay = int(task.split(\"=\")[1].strip())\n            delay_task(delay)\n        elif task.startswith(\"MouseMove =\"):\n            coordinates = task.split(\"=\")[1].strip()\n            move_mouse_to_coordinates_raw(coordinates)\n        elif task == \"TakeScreenshot\":\n            take_screenshot_task()\n        elif task == \"PageUp\":\n            pyautogui.press(\"pageup\")\n        elif task == \"PageDown\":\n            pyautogui.press(\"pagedown\")\n        elif task.startswith(\"OOB_\"):\n            oob_tasks.append(task)\n        else:\n            print(f\"Unknown task: {task}\")\n            \n        time.sleep(0.2)\n\ndef execute_oob_tasks_with_delay():\n    \"\"\"\n    Executes the saved OOB tasks after a delay.\n    \"\"\"\n    time.sleep(0.1)  # Short delay to ensure the LLM's response has completed\n    execute_oob_tasks()\n\ndef execute_oob_tasks():\n    \"\"\"\n    Executes the saved OOB tasks.\n    \"\"\"\n    with open(oob_tasks_file, 'r') as f:\n        oob_tasks = json.load(f)\n\n    for task in oob_tasks:\n        task = task.strip()\n        if task.startswith(\"OOB_MouseMover =\"):\n            image_name = task.split(\"=\")[1].strip()\n            move_mouse_to_image(image_name)\n        elif task.startswith(\"OOB_MouseClick =\"):\n            value = task.split(\"=\")[1].strip()\n            if \",\" in value:  # Check if the value contains coordinates\n                coordinates = value\n                left_click_coordinates_raw(coordinates)\n            else:  # Otherwise, treat it as an image name\n                image_name = value\n                left_click_image(image_name)\n        elif task.startswith(\"OOB_TextInput =\"):\n            text = task.split(\"=\")[1].strip()\n            text_input(text)\n        elif task.startswith(\"OOB_SpecialKey =\"):\n            key = task.split(\"=\")[1].strip()\n            special_key(key)\n        elif task.startswith(\"OOB_FunctionKey =\"):\n            key = task.split(\"=\")[1].strip()\n            function_key(key)\n        elif task.startswith(\"OOB_KeyCombo =\"):\n            combo = task.split(\"=\")[1].strip()\n            key_combo(combo)\n        elif task.startswith(\"OOB_Delay =\"):\n            delay = int(task.split(\"=\")[1].strip())\n            delay_task(delay)\n        elif task.startswith(\"OOB_MouseMove =\"):\n            coordinates = task.split(\"=\")[1].strip()\n            move_mouse_to_coordinates_raw(coordinates)\n        elif task.startswith(\"OOB_MouseClick =\"):\n            coordinates = task.split(\"=\")[1].strip()\n            left_click_coordinates_raw(coordinates)\n        elif task == \"OOB_TakeScreenshot\":\n            take_screenshot_task()\n        elif task == \"OOB_PageUp\":\n            pyautogui.press(\"pageup\")\n        elif task == \"OOB_PageDown\":\n            pyautogui.press(\"pagedown\")\n        else:\n            print(f\"Unknown OOB task: {task}\")\n            \n        time.sleep(0.2)\n\ndef delay_task(delay):\n    \"\"\"\n    Adds a delay in milliseconds.\n    \"\"\"\n    time.sleep(delay / 1000.0)\n\ndef move_mouse_to_coordinates_raw(coordinates):\n    \"\"\"\n    Moves the mouse to the specified raw coordinates on the specified monitor.\n\n    Args:\n        coordinates (str): A string containing the monitor index and coordinates in the format \"monitor_index,x,y\".\n    \"\"\"\n    monitor_index, x, y = map(int, coordinates.split(\",\"))\n    monitors = screeninfo.get_monitors()\n\n    if monitor_index >= len(monitors):\n        raise ValueError(f\"Monitor index {monitor_index} is out of range. There are only {len(monitors)} monitors.\")\n\n    monitor = monitors[monitor_index]\n    monitor_x, monitor_y = monitor.x, monitor.y\n    absolute_x = monitor_x + x\n    absolute_y = monitor_y + y\n    pyautogui.moveTo(absolute_x, absolute_y)\n    print(f\"Mouse moved to coordinates: ({absolute_x}, {absolute_y}) on monitor {monitor_index}\")\n\ndef left_click_coordinates_raw(coordinates):\n    \"\"\"\n    Moves the mouse to the specified raw coordinates on the specified monitor and performs a left click.\n\n    Args:\n        coordinates (str): A string containing the monitor index and coordinates in the format \"monitor_index,x,y\".\n    \"\"\"\n    monitor_index, x, y = map(int, coordinates.split(\",\"))\n    monitors = screeninfo.get_monitors()\n\n    if monitor_index >= len(monitors):\n        raise ValueError(f\"Monitor index {monitor_index} is out of range. There are only {len(monitors)} monitors.\")\n\n    monitor = monitors[monitor_index]\n    monitor_x, monitor_y = monitor.x, monitor.y\n    absolute_x = monitor_x + x\n    absolute_y = monitor_y + y\n    pyautogui.moveTo(absolute_x, absolute_y)\n    pyautogui.click()\n    print(f\"Mouse clicked at coordinates: ({absolute_x}, {absolute_y}) on monitor {monitor_index}\")\n\ndef take_screenshot_task():\n    \"\"\"\n    Takes a screenshot using the current values from the Gradio interface.\n    \"\"\"\n    take_screenshot(global_vars[\"global_monitor_index\"], global_vars[\"global_text_queries\"], global_vars[\"global_score_threshold\"], global_vars[\"global_vision_model_question\"], global_vars[\"global_full_image_vision_model_question\"])\n\ndef move_mouse_to_image(image_name):\n    \"\"\"\n    Moves the mouse to the center of the box that defines the image.\n    \"\"\"\n    results_json_path = \"extensions/Lucid_Autonomy/ImageOutputTest/results.json\"\n    with open(results_json_path, 'r') as f:\n        results = json.load(f)\n\n    for result in results:\n        if result[\"image_name\"] == image_name:\n            coordinates = result[\"coordinates\"]\n            monitor = get_monitor_info(MONITOR_INDEX)\n            move_mouse_to_coordinates(monitor, coordinates)\n            break\n    else:\n        print(f\"Image name '{image_name}' not found in the results.\")\n\ndef left_click_image(image_name):\n    \"\"\"\n    Moves the mouse to the center of the box that defines the image and performs a left click.\n    \"\"\"\n    results_json_path = \"extensions/Lucid_Autonomy/ImageOutputTest/results.json\"\n    with open(results_json_path, 'r') as f:\n        results = json.load(f)\n\n    for result in results:\n        if result[\"image_name\"] == image_name:\n            coordinates = result[\"coordinates\"]\n            monitor = get_monitor_info(MONITOR_INDEX)\n            move_mouse_to_coordinates(monitor, coordinates)\n            pyautogui.click()  # Perform a left click\n            break\n    else:\n        print(f\"Image name '{image_name}' not found in the results.\")\n\ndef move_mouse_to_coordinates(monitor, coordinates):\n    \"\"\"\n    Moves the mouse cursor to the specified coordinates on the monitor.\n\n    Args:\n        monitor (dict): A dictionary containing the monitor's information.\n        coordinates (list): A list containing the coordinates [x1, y1, x2, y2].\n    \"\"\"\n    x1, y1, x2, y2 = coordinates\n    monitor_x, monitor_y = monitor.x, monitor.y\n    center_x = monitor_x + (x1 + x2) // 2\n    center_y = monitor_y + (y1 + y2) // 2\n    pyautogui.moveTo(center_x, center_y)\n    print(f\"Mouse moved to coordinates: ({center_x}, {center_y})\")\n\ndef get_monitor_info(monitor_index):\n    \"\"\"\n    Gets information about the specified monitor.\n\n    Args:\n        monitor_index (int): The index of the monitor to get information about.\n\n    Returns:\n        dict: A dictionary containing the monitor's information.\n    \"\"\"\n    monitors = screeninfo.get_monitors()\n\n    if monitor_index >= len(monitors):\n        raise ValueError(f\"Monitor index {monitor_index} is out of range. There are only {len(monitors)} monitors.\")\n\n    return monitors[monitor_index]\n\ndef history_modifier(history):\n    \"\"\"\n    Modifies the chat history before the text generation in chat mode begins.\n    \"\"\"\n    # Clean the internal history\n    history[\"internal\"] = clean_chat_history(history[\"internal\"])\n    # Clean the visible history\n    history[\"visible\"] = clean_chat_history(history[\"visible\"])\n\n    # Inject the latest compressedresults.json into the internal history\n    compressed_results_json_path = \"extensions/Lucid_Autonomy/ImageOutputTest/compressedresults.json\"\n    with open(compressed_results_json_path, 'r') as f:\n        compressed_results = json.load(f)\n    latest_compressed_results = json.dumps(compressed_results, indent=4)\n    history[\"internal\"].append([\"\", f\"<START_COMPRESSED_RESULTS>\\n{latest_compressed_results}\\n<END_COMPRESSED_RESULTS>\"])\n\n    return history\n\ndef clean_chat_history(history):\n    \"\"\"\n    Cleans the chat history by removing all but the latest instance of the contents between the bookends.\n    \"\"\"\n    START_BOOKEND = \"<START_COMPRESSED_RESULTS>\"\n    END_BOOKEND = \"<END_COMPRESSED_RESULTS>\"\n\n    # Find all instances of the bookends\n    bookend_indices = []\n    for i, entry in enumerate(history):\n        if isinstance(entry, list) and len(entry) == 2:\n            if START_BOOKEND in entry[1] and END_BOOKEND in entry[1]:\n                bookend_indices.append(i)\n\n    # If there are multiple instances, remove all but the last one\n    if len(bookend_indices) > 1:\n        for i in bookend_indices[:-1]:\n            history[i][1] = re.sub(f\"{START_BOOKEND}.*?{END_BOOKEND}\", \"\", history[i][1], flags=re.DOTALL)\n\n    return history\n\ndef append_compressed_results(output, compressed_results):\n    \"\"\"\n    Appends the compressed results to the output.\n    \"\"\"\n    START_BOOKEND = \"<START_COMPRESSED_RESULTS>\"\n    END_BOOKEND = \"<END_COMPRESSED_RESULTS>\"\n    output += f\"\\n{START_BOOKEND}\\n{json.dumps(compressed_results, indent=4)}\\n{END_BOOKEND}\"\n    return output\n\ndef text_input(text):\n    \"\"\"\n    Types the given text, replacing '/n' with newline characters.\n    \"\"\"\n    text = text.replace(\"/n\", \"\\n\")\n    pyautogui.typewrite(text)\n\ndef special_key(key):\n    \"\"\"\n    Presses the specified special key.\n    \"\"\"\n    if key == \"enter\":\n        pyautogui.press(\"enter\")\n    elif key == \"space\":\n        pyautogui.press(\"space\")\n    elif key == \"backspace\":\n        pyautogui.press(\"backspace\")\n    elif key == \"tab\":\n        pyautogui.press(\"tab\")\n    elif key == \"escape\":\n        pyautogui.press(\"esc\")\n    elif key == \"shift\":\n        pyautogui.press(\"shift\")\n    elif key == \"ctrl\":\n        pyautogui.press(\"ctrl\")\n    elif key == \"alt\":\n        pyautogui.press(\"alt\")\n    else:\n        print(f\"Unsupported special key: {key}\")\n\ndef function_key(key):\n    \"\"\"\n    Presses the specified function key.\n    \"\"\"\n    if key.startswith(\"f\") and key[1:].isdigit():\n        pyautogui.press(key)\n    else:\n        print(f\"Unsupported function key: {key}\")\n\ndef key_combo(combo):\n    \"\"\"\n    Presses the specified key combination.\n    \"\"\"\n    keys = combo.split(\"+\")\n    for key in keys:\n        if key in [\"ctrl\", \"alt\", \"shift\"]:\n            pyautogui.keyDown(key)\n        else:\n            pyautogui.press(key)\n    for key in keys:\n        if key in [\"ctrl\", \"alt\", \"shift\"]:\n            pyautogui.keyUp(key)\n\nif __name__ == \"__main__\":\n    # Launch the Gradio interface with the specified UI components\n    gr.Interface(\n        fn=ui,\n        inputs=None,\n        outputs=\"ui\"\n    ).launch()",
            "Thank you for sharing the code. Here are my notes and observations:\n\n### Notes and Observations\n\n1. **Imports and Dependencies**:\n   - The code imports various libraries such as `gradio`, `pyautogui`, `screeninfo`, `transformers`, `PIL`, `torch`, `os`, `shutil`, `re`, `json`, `time`, and `threading`.\n   - These libraries are used for creating a user interface, automating mouse and keyboard actions, processing images, and handling file operations.\n\n2. **Configurable Variables**:\n   - The code defines several configurable variables such as `MONITOR_INDEX`, `TEXT_QUERIES`, `SCORE_THRESHOLD`, `VISION_MODEL_QUESTION`, and `FULL_IMAGE_VISION_MODEL_QUESTION`.\n   - These variables allow for customization of the monitor to capture, text queries for object detection, score threshold for filtering predictions, and questions for the vision model.\n\n3. **Model Loading and Unloading**:\n   - The code includes functions for loading and unloading the `Owlv2` and `MiniCPM-Llama3` models.\n   - The models are loaded on-demand and unloaded after use to free up GPU memory.\n\n4. **Screenshot and Image Processing**:\n   - The `take_screenshot` function captures a screenshot of a specified monitor and processes the image using the `Owlv2` model to detect objects.\n   - The detected objects are cropped and processed with the `MiniCPM-Llama3` model to generate descriptions.\n   - The results are saved in JSON format and the output directory is cleared before saving new results.\n\n5. **Gradio Interface**:\n   - The `ui` function creates a custom Gradio interface with input fields for configuring the monitor index, text queries, score threshold, and vision model questions.\n   - The interface includes a button to trigger the screenshot and image processing functions.\n\n6. **Task Execution**:\n   - The `output_modifier` function modifies the LLM output before it is presented in the UI.\n   - It searches for the \"Autonomy_Tasks:\" trigger phrase in the LLM's output and executes the tasks listed in the tasks section.\n   - The tasks include mouse movements, clicks, text input, special key presses, function key presses, key combinations, delays, and taking screenshots.\n   - The tasks are executed with a short delay to ensure the LLM's response has completed.\n\n7. **Out-of-Band (OOB) Tasks**:\n   - The code includes functions for executing OOB tasks after a delay.\n   - OOB tasks are saved to a JSON file and executed separately from the main tasks.\n\n8. **Helper Functions**:\n   - The code includes several helper functions for moving the mouse, clicking, typing text, pressing keys, and handling delays.\n   - These functions use the `pyautogui` library to automate mouse and keyboard actions.\n\n### Contextualization\n\nThe code is a comprehensive implementation of the `Lucid_Autonomy` project, which aims to enable a Large Language Model (LLM) to interact autonomously with a computer. The code leverages various models and tools to detect objects on the screen, process images, and perform actions such as moving the mouse, clicking, typing text, and pressing keys. The Gradio interface provides a user-friendly way to configure and trigger these actions.\n\n### Next Steps\n\nPlease let me know what specific tasks or issues you would like me to focus on next. I can help with code review, debugging, optimization, or any other aspect of the project."
        ]
    ],
    "visible": [
        [
            "",
            "How can I help you today?"
        ],
        [
            "Hello how are you functioning?  I&#x27;m going to give you a summary of the project we have been working on and I&#x27;ll give you the code in the next message.\n\n### Summary of Lucid_Autonomy\n\n#### Overview\n`Lucid_Autonomy` is an extension designed to enable a Large Language Model (LLM) to interact autonomously with a computer. It leverages various models and tools to detect objects on the screen, process images, and perform actions such as moving the mouse, clicking, typing text, and pressing keys.\n\n#### Key Components\n1. **Models**:\n   - **Owlv2 Model**: Used for object detection based on text queries.\n   - **MiniCPM-Llama3-V-2_5 Model**: Used for processing images and generating descriptions.\n\n2. **Tools**:\n   - **Gradio Interface**: Provides a user interface for configuring and triggering actions.\n   - **PyAutoGUI**: Used for automating mouse and keyboard actions.\n   - **Screeninfo**: Used for getting information about connected monitors.\n\n3. **Functions**:\n   - **take_screenshot**: Captures a screenshot of a specified monitor.\n   - **query_image**: Processes the screenshot using the Owlv2 model to detect objects.\n   - **crop_images**: Crops out detected objects and processes them with the MiniCPM model.\n   - **process_with_vision_model**: Processes images with the MiniCPM model and returns descriptions.\n   - **execute_tasks**: Executes tasks listed in the LLM&#x27;s output, such as moving the mouse, clicking, typing text, and pressing keys.\n   - **execute_oob_tasks**: Executes out-of-band (OOB) tasks after a delay.\n\n#### Ultimate Intent\nThe ultimate intent of `Lucid_Autonomy` is to provide the LLM with &quot;eyes&quot; and &quot;hands&quot; to interact autonomously with a computer. This includes the ability to:\n- Take screenshots and detect objects on the screen.\n- Move the mouse and click on specific coordinates or detected objects.\n- Type text and press keys.\n- Perform complex tasks such as navigating a web browser or interacting with a user interface.\n\n#### How It Fits into Textgen\n`Lucid_Autonomy` is designed to work within the `textgen` ecosystem, which is a powerful web-based interface for running large language models (LLMs) locally. The extension enhances the capabilities of `textgen` by allowing the LLM to interact with the user&#x27;s computer, effectively giving it the ability to perform tasks that would otherwise require human intervention.\n\n### Conclusion\n`Lucid_Autonomy` is a powerful extension that enhances the capabilities of LLMs by enabling them to interact autonomously with a computer. It fits seamlessly into the `textgen` ecosystem, providing a robust framework for automating tasks and enhancing user interaction. When starting a new conversation, providing clear context, relevant code snippets, goals, and constraints will help ensure effective and efficient assistance.",
            "Thank you for the detailed summary of the `Lucid_Autonomy` project. It sounds like a sophisticated and innovative extension that significantly enhances the capabilities of Large Language Models. I&#x27;m ready to review the code and provide any assistance you need. Please go ahead and share the code, and let me know if there are any specific areas you&#x27;d like me to focus on or any particular issues you&#x27;re encountering."
        ],
        [
            "Here is the code, I want you to write notes and observations; contextualize the code, and I&#x27;ll tell you what I want you to do next:\n\nimport gradio as gr\nimport json\nimport pyautogui\nimport screeninfo\nfrom transformers import Owlv2Processor, Owlv2ForObjectDetection, AutoModel, AutoTokenizer\nfrom PIL import Image, ImageDraw\nimport torch\nimport os\nimport shutil\nimport re  # Import the re module for regular expression operations\nimport json\nimport time\nimport threading\n\noob_tasks_file = &quot;extensions/Lucid_Autonomy/oob_tasks.json&quot;\n\n# Configurable variables\nMONITOR_INDEX = 1  # Index of the monitor to capture (0 for the first monitor, 1 for the second monitor, etc.)\nTEXT_QUERIES = &quot;colored hyperlink text,clickable icons,text bar field,clickable UI buttons,UI tabs,Interactive UI Elements,blue text hyperlink&quot;  # Comma-separated text queries for object detection\nSCORE_THRESHOLD = 0.24  # Score threshold for filtering low-probability predictions\nVISION_MODEL_QUESTION = &quot;Keep your response to less than 2 sentences: What is the text in this image or what does this image describe? The AI is aiding in helping those with visual handicaps so the AI will focus on providing all of the relevant information for the listener.&quot;\nFULL_IMAGE_VISION_MODEL_QUESTION = &quot;This is a computer screenshot, identify all article text if the screen is displaying an article.  Describe what you see in this image, if you see UI elements describe them and their general location and if you see text read the text verbatim.  You are aiding in helping those with visual handicaps so you need to try and focus on providing all of the relevant text on a screen and contextualizing it for the listener.&quot;\n\n# Use GPU if available\nif torch.cuda.is_available():\n    device = torch.device(&quot;cuda&quot;)\nelse:\n    device = torch.device(&quot;cpu&quot;)\n\n# Global variables for models\nowlv2_model = None\nowlv2_processor = None\nminicpm_llama_model = None\nminicpm_llama_tokenizer = None\n\nVISION_MODEL_ID = &quot;/home/myself/Desktop/miniCPM_llava3V/MiniCPM-V-2_6&quot;\n\n#google/owlv2-base-patch16-ensemble\n#google/owlv2-base-patch16\n#google/owlv2-base-patch16-finetuned\n\ndef load_owlv2_model():\n    global owlv2_model, owlv2_processor\n    owlv2_model = Owlv2ForObjectDetection.from_pretrained(&quot;google/owlv2-base-patch16-ensemble&quot;, trust_remote_code=True).to(device)\n    owlv2_processor = Owlv2Processor.from_pretrained(&quot;google/owlv2-base-patch16-ensemble&quot;, trust_remote_code=True)\n    print(&quot;Owlv2 model loaded.&quot;)\n\ndef unload_owlv2_model():\n    global owlv2_model, owlv2_processor\n    if owlv2_model is not None:\n        del owlv2_model\n        del owlv2_processor\n        torch.cuda.empty_cache()\n        print(&quot;Owlv2 model unloaded.&quot;)\n\ndef load_minicpm_llama_model():\n    global minicpm_llama_model, minicpm_llama_tokenizer\n    if &quot;int4&quot; in VISION_MODEL_ID:\n        # Load the 4-bit quantized model and tokenizer\n        minicpm_llama_model = AutoModel.from_pretrained(\n            VISION_MODEL_ID,\n            device_map={&quot;&quot;: device},\n            trust_remote_code=True,\n            torch_dtype=torch.float16  # Use float16 as per the example code for 4-bit models\n        ).eval()\n        minicpm_llama_tokenizer = AutoTokenizer.from_pretrained(\n            VISION_MODEL_ID,\n            trust_remote_code=True\n        )\n        print(&quot;MiniCPM-Llama3 4-bit quantized model loaded on-demand.&quot;)\n    else:\n        # Load the standard model and tokenizer\n        minicpm_llama_model = AutoModel.from_pretrained(\n            VISION_MODEL_ID,\n            device_map={&quot;&quot;: device},  # Use the specified CUDA device\n            trust_remote_code=True,\n            torch_dtype=torch.bfloat16\n        ).eval()\n        minicpm_llama_tokenizer = AutoTokenizer.from_pretrained(\n            VISION_MODEL_ID,\n            trust_remote_code=True\n        )\n        print(&quot;MiniCPM-Llama3 standard model loaded on-demand.&quot;)\n\ndef unload_minicpm_llama_model():\n    global minicpm_llama_model, minicpm_llama_tokenizer\n    if minicpm_llama_model is not None:\n        del minicpm_llama_model\n        del minicpm_llama_tokenizer\n        torch.cuda.empty_cache()\n        print(&quot;MiniCPM-Llama3 model unloaded.&quot;)\n\ndef take_screenshot(monitor_index, text_queries, score_threshold, vision_model_question, full_image_vision_model_question, output_filename=&quot;screenshot.png&quot;):\n    &quot;&quot;&quot;\n    Takes a screenshot of a specific monitor in a multi-monitor setup.\n\n    Args:\n        monitor_index (int): The index of the monitor to capture.\n        text_queries (str): Comma-separated text queries for object detection.\n        score_threshold (float): The score threshold for filtering low-probability predictions.\n        vision_model_question (str): The question to ask the vision model for cropped images.\n        full_image_vision_model_question (str): The question to ask the vision model for the full image.\n        output_filename (str): The filename to save the screenshot as (default is &quot;screenshot.png&quot;).\n    &quot;&quot;&quot;\n    monitor_index = int(monitor_index)\n    score_threshold = float(score_threshold)\n\n    # Get information about all monitors\n    monitors = screeninfo.get_monitors()\n\n    if monitor_index &gt;= len(monitors):\n        raise ValueError(f&quot;Monitor index {monitor_index} is out of range. There are only {len(monitors)} monitors.&quot;)\n\n    # Get the region of the specified monitor\n    monitor = monitors[monitor_index]\n    region = (monitor.x, monitor.y, monitor.width, monitor.height)\n\n    # Take the screenshot\n    output_dir = &quot;extensions/Lucid_Autonomy&quot;\n    os.makedirs(output_dir, exist_ok=True)\n    screenshot_path = os.path.join(output_dir, output_filename)\n    screenshot = pyautogui.screenshot(region=region)\n    screenshot.save(screenshot_path)\n    print(f&quot;Screenshot saved as {screenshot_path}&quot;)\n\n    # Process the image using the Owlv2 model\n    annotated_image_path, human_annotated_image_path, json_output_path = query_image(screenshot_path, text_queries, score_threshold)\n\n    # Load the miniCPM-lama3-v-2_5 model\n    load_minicpm_llama_model()\n\n    # Process the entire screenshot with the minicpm model\n    full_image_response = process_with_vision_model(annotated_image_path, full_image_vision_model_question)\n\n    # Unload the miniCPM-lama3-v-2_5 model after processing the full image\n    unload_minicpm_llama_model()\n\n    # Create the output directory if it doesn&#x27;t exist\n    output_dir = &quot;extensions/Lucid_Autonomy/ImageOutputTest&quot;\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Clear the output directory\n    for filename in os.listdir(output_dir):\n        file_path = os.path.join(output_dir, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f&quot;Failed to delete {file_path}. Reason: {e}&quot;)\n\n    # Crop the detected objects and save them to the output directory\n    crop_images(annotated_image_path, json_output_path, output_dir, full_image_response)\n\n    print(&quot;Processing complete. Check the output directory for the cropped images and results.&quot;)\n\ndef query_image(img_path, text_queries, score_threshold):\n    &quot;&quot;&quot;\n    Processes the image using the Owlv2 model to detect objects based on text queries.\n\n    Args:\n        img_path (str): The path to the input image.\n        text_queries (str): Comma-separated text queries for object detection.\n        score_threshold (float): The score threshold for filtering low-probability predictions.\n    &quot;&quot;&quot;\n    load_owlv2_model()\n\n    img = Image.open(img_path)\n    text_queries = text_queries.split(&quot;,&quot;)\n\n    size = max(img.size)\n    target_sizes = torch.Tensor([[size, size]])\n    inputs = owlv2_processor(text=text_queries, images=img, return_tensors=&quot;pt&quot;).to(device)\n\n    with torch.no_grad():\n        outputs = owlv2_model(**inputs)\n\n    outputs.logits = outputs.logits.cpu()\n    outputs.pred_boxes = outputs.pred_boxes.cpu()\n    results = owlv2_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes)\n    boxes, scores, labels = results[0][&quot;boxes&quot;], results[0][&quot;scores&quot;], results[0][&quot;labels&quot;]\n\n    result_labels = []\n    img_pil = img.copy()\n    draw = ImageDraw.Draw(img_pil)\n\n    human_img_pil = img.copy()\n    human_draw = ImageDraw.Draw(human_img_pil)\n\n    for box, score, label in zip(boxes, scores, labels):\n        box = [int(i) for i in box.tolist()]\n        if score &lt; score_threshold:\n            continue\n        result_labels.append([box, text_queries[label.item()]])\n        # Do not draw the text label within the bounding box\n        # draw.text((box[0], box[1]), text_queries[label.item()], fill=&quot;red&quot;)\n\n        # Draw the boxes for the human-annotated image\n        human_draw.rectangle(box, outline=&quot;red&quot;, width=2)\n\n    # Save the annotated image for debugging\n    annotated_image_path = &quot;extensions/Lucid_Autonomy/annotated_image.png&quot;\n    img_pil.save(annotated_image_path)\n\n    # Save the human-annotated image for human inspection\n    human_annotated_image_path = &quot;extensions/Lucid_Autonomy/Human_annotated_image.png&quot;\n    human_img_pil.save(human_annotated_image_path)\n\n    # Save the JSON output to a file\n    json_output_path = &quot;extensions/Lucid_Autonomy/output.json&quot;\n    with open(json_output_path, &#x27;w&#x27;) as f:\n        json.dump(result_labels, f)\n\n    unload_owlv2_model()\n\n    return annotated_image_path, human_annotated_image_path, json_output_path\n\ndef crop_images(image_path, json_output_path, output_dir=&quot;extensions/Lucid_Autonomy/ImageOutputTest&quot;, full_image_response=&quot;&quot;):\n    &quot;&quot;&quot;\n    Crops out the detected objects from the input image based on the bounding box coordinates.\n\n    Args:\n        image_path (str): The path to the input image.\n        json_output_path (str): The path to the JSON file containing the bounding box coordinates.\n        output_dir (str): The directory to save the cropped images (default is &quot;extensions/Lucid_Autonomy/ImageOutputTest&quot;).\n        full_image_response (str): The response from the vision model for the full image.\n    &quot;&quot;&quot;\n    with open(json_output_path, &#x27;r&#x27;) as f:\n        data = json.load(f)\n\n    img = Image.open(image_path)\n\n    results = []\n\n    # Load the miniCPM-lama3-v-2_5 model once before processing the images\n    load_minicpm_llama_model()\n\n    for i, item in enumerate(data):\n        box = item[0]\n        label = item[1]\n        cropped_img = img.crop(box)\n        cropped_img_path = f&quot;{output_dir}/{label}_{i}.png&quot;\n        cropped_img.save(cropped_img_path)\n\n        # Process the cropped image with the miniCPM-lama3-v-2_5 model\n        vision_model_response = process_with_vision_model(cropped_img_path, VISION_MODEL_QUESTION)\n\n        # Store the results in a structured format\n        results.append({\n            &quot;image_name&quot;: f&quot;{label}_{i}.png&quot;,\n            &quot;coordinates&quot;: box,\n            &quot;description&quot;: vision_model_response,\n            &quot;children&quot;: []\n        })\n\n    # Unload the miniCPM-lama3-v-2_5 model after processing all images\n    unload_minicpm_llama_model()\n\n    # Determine parent-child relationships\n    for i, result in enumerate(results):\n        for j, other_result in enumerate(results):\n            if i != j:\n                box1 = result[&quot;coordinates&quot;]\n                box2 = other_result[&quot;coordinates&quot;]\n                center_x1 = (box1[0] + box1[2]) // 2\n                center_y1 = (box1[1] + box1[3]) // 2\n                if box2[0] &lt;= center_x1 &lt;= box2[2] and box2[1] &lt;= center_y1 &lt;= box2[3]:\n                    other_result[&quot;children&quot;].append(result[&quot;image_name&quot;])\n\n    # Save the results to a JSON file\n    results_json_path = f&quot;{output_dir}/results.json&quot;\n    with open(results_json_path, &#x27;w&#x27;) as f:\n        json.dump(results, f, indent=4)\n\n    # Create the compressed results JSON file\n    compressed_results_json_path = f&quot;{output_dir}/compressedresults.json&quot;\n    compressed_results = [{&quot;image_name&quot;: result[&quot;image_name&quot;], &quot;description&quot;: result[&quot;description&quot;], &quot;children&quot;: result[&quot;children&quot;]} for result in results]\n    with open(compressed_results_json_path, &#x27;w&#x27;) as f:\n        json.dump(compressed_results, f, indent=4)\n\n    # Add the full image response to the results\n    if full_image_response:\n        results.append({\n            &quot;image_name&quot;: &quot;full_image.png&quot;,\n            &quot;coordinates&quot;: [],\n            &quot;description&quot;: full_image_response,\n            &quot;children&quot;: []\n        })\n        compressed_results.append({\n            &quot;image_name&quot;: &quot;full_image.png&quot;,\n            &quot;description&quot;: full_image_response,\n            &quot;children&quot;: []\n        })\n\n        # Save the updated results to the JSON files\n        with open(results_json_path, &#x27;w&#x27;) as f:\n            json.dump(results, f, indent=4)\n        with open(compressed_results_json_path, &#x27;w&#x27;) as f:\n            json.dump(compressed_results, f, indent=4)\n\ndef process_with_vision_model(image_path, question):\n    &quot;&quot;&quot;\n    Processes the image with the miniCPM-lama3-v-2_5 model and returns the response.\n\n    Args:\n        image_path (str): The path to the input image.\n        question (str): The question to ask the vision model.\n\n    Returns:\n        str: The response from the vision model.\n    &quot;&quot;&quot;\n    image = Image.open(image_path).convert(&quot;RGB&quot;)\n    messages = [\n        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{question}&quot;}\n    ]\n    # Define the generation arguments\n    generation_args = {\n        &quot;max_new_tokens&quot;: 1024,\n        &quot;repetition_penalty&quot;: 1.05,\n        &quot;num_beams&quot;: 3,\n        &quot;top_p&quot;: 0.9,\n        &quot;top_k&quot;: 1,\n        &quot;temperature&quot;: 0.1,\n        &quot;sampling&quot;: True,\n    }\n    if &quot;int4&quot; in &quot;openbmb/MiniCPM-Llama3-V-2_5&quot;:\n        # Disable streaming for the 4-bit model\n        generation_args[&quot;stream&quot;] = False\n        # Use the model.chat method with streaming enabled\n        vision_model_response = &quot;&quot;\n        for new_text in minicpm_llama_model.chat(\n                image=image,\n                msgs=messages,\n                tokenizer=minicpm_llama_tokenizer,\n                **generation_args\n        ):\n            vision_model_response += new_text\n            print(new_text, flush=True, end=&#x27;&#x27;)\n\n    else:\n        minicpm_llama_model.to(device)\n        vision_model_response = minicpm_llama_model.chat(\n            image=image,\n            msgs=messages,\n            tokenizer=minicpm_llama_tokenizer,\n            **generation_args\n        )\n    return vision_model_response\n\n# Global variables for Gradio inputs\nglobal_vars = {\n    &quot;global_monitor_index&quot;: MONITOR_INDEX,\n    &quot;global_text_queries&quot;: TEXT_QUERIES,\n    &quot;global_score_threshold&quot;: SCORE_THRESHOLD,\n    &quot;global_vision_model_question&quot;: VISION_MODEL_QUESTION,\n    &quot;global_full_image_vision_model_question&quot;: FULL_IMAGE_VISION_MODEL_QUESTION\n}\n\ndef ui():\n    &quot;&quot;&quot;\n    Creates custom gradio elements when the UI is launched.\n    &quot;&quot;&quot;\n    with gr.Blocks() as demo:\n        with gr.Row():\n            with gr.Column():\n                monitor_index = gr.Textbox(label=&quot;Monitor Index&quot;, value=str(MONITOR_INDEX))\n                text_queries = gr.Textbox(label=&quot;Text Queries&quot;, value=TEXT_QUERIES)\n                score_threshold = gr.Textbox(label=&quot;Score Threshold&quot;, value=str(SCORE_THRESHOLD))\n                vision_model_question = gr.Textbox(label=&quot;Vision Model Question&quot;, value=VISION_MODEL_QUESTION)\n                full_image_vision_model_question = gr.Textbox(label=&quot;Full Image Vision Model Question&quot;, value=FULL_IMAGE_VISION_MODEL_QUESTION)\n                take_screenshot_button = gr.Button(value=&quot;Take Screenshot&quot;)\n\n        # Update global variables when the user changes the input fields\n        monitor_index.change(lambda x: global_vars.update({&quot;global_monitor_index&quot;: int(x)}), inputs=monitor_index, outputs=None)\n        text_queries.change(lambda x: global_vars.update({&quot;global_text_queries&quot;: x}), inputs=text_queries, outputs=None)\n        score_threshold.change(lambda x: global_vars.update({&quot;global_score_threshold&quot;: float(x)}), inputs=score_threshold, outputs=None)\n        vision_model_question.change(lambda x: global_vars.update({&quot;global_vision_model_question&quot;: x}), inputs=vision_model_question, outputs=None)\n        full_image_vision_model_question.change(lambda x: global_vars.update({&quot;global_full_image_vision_model_question&quot;: x}), inputs=full_image_vision_model_question, outputs=None)\n\n        take_screenshot_button.click(\n            fn=take_screenshot,\n            inputs=[monitor_index, text_queries, score_threshold, vision_model_question, full_image_vision_model_question],\n            outputs=None\n        )\n\n    return demo\n\ndef input_modifier(user_input, state):\n    &quot;&quot;&quot;\n    Modifies the user input before it is processed by the LLM.\n    &quot;&quot;&quot;\n    return user_input\n\ndef output_modifier(output, state):\n    &quot;&quot;&quot;\n    Modifies the LLM output before it is presented in the UI.\n    &quot;&quot;&quot;\n    global oob_tasks\n    oob_tasks = []\n\n    # Search for the &quot;Autonomy_Tasks:&quot; trigger phrase in the LLM&#x27;s output\n    if &quot;Autonomy_Tasks:&quot; in output:\n        # Clean the chat history\n        state[&quot;history&quot;] = history_modifier(state[&quot;history&quot;])\n        # Extract tasks and execute them\n        tasks_section = output.split(&quot;Autonomy_Tasks:&quot;)[-1].strip()\n        if tasks_section:\n            execute_tasks(tasks_section)\n        # Append new compressed results\n        compressed_results_json_path = &quot;extensions/Lucid_Autonomy/ImageOutputTest/compressedresults.json&quot;\n        with open(compressed_results_json_path, &#x27;r&#x27;) as f:\n            compressed_results = json.load(f)\n        output = append_compressed_results(output, compressed_results)\n\n        # Write OOB tasks to a JSON file\n        with open(oob_tasks_file, &#x27;w&#x27;) as f:\n            json.dump(oob_tasks, f)\n\n        # Trigger the execution of OOB tasks after a delay\n        threading.Thread(target=execute_oob_tasks_with_delay).start()\n\n    return output\n\noob_tasks = []\n\ndef execute_tasks(tasks_section):\n    &quot;&quot;&quot;\n    Executes the tasks listed in the tasks_section.\n    &quot;&quot;&quot;\n    global oob_tasks\n    oob_tasks = []\n    tasks = tasks_section.split(&quot;\\n&quot;)\n    for task in tasks:\n        task = task.strip()\n        if task.startswith(&quot;MouseMover =&quot;):\n            image_name = task.split(&quot;=&quot;)[1].strip()\n            move_mouse_to_image(image_name)\n        elif task.startswith(&quot;MouseClick =&quot;):\n            value = task.split(&quot;=&quot;)[1].strip()\n            if &quot;,&quot; in value:  # Check if the value contains coordinates\n                coordinates = value\n                left_click_coordinates_raw(coordinates)\n            else:  # Otherwise, treat it as an image name\n                image_name = value\n                left_click_image(image_name)\n        elif task.startswith(&quot;TextInput =&quot;):\n            text = task.split(&quot;=&quot;)[1].strip()\n            text_input(text)\n        elif task.startswith(&quot;SpecialKey =&quot;):\n            key = task.split(&quot;=&quot;)[1].strip()\n            special_key(key)\n        elif task.startswith(&quot;FunctionKey =&quot;):\n            key = task.split(&quot;=&quot;)[1].strip()\n            function_key(key)\n        elif task.startswith(&quot;KeyCombo =&quot;):\n            combo = task.split(&quot;=&quot;)[1].strip()\n            key_combo(combo)\n        elif task.startswith(&quot;Delay =&quot;):\n            delay = int(task.split(&quot;=&quot;)[1].strip())\n            delay_task(delay)\n        elif task.startswith(&quot;MouseMove =&quot;):\n            coordinates = task.split(&quot;=&quot;)[1].strip()\n            move_mouse_to_coordinates_raw(coordinates)\n        elif task == &quot;TakeScreenshot&quot;:\n            take_screenshot_task()\n        elif task == &quot;PageUp&quot;:\n            pyautogui.press(&quot;pageup&quot;)\n        elif task == &quot;PageDown&quot;:\n            pyautogui.press(&quot;pagedown&quot;)\n        elif task.startswith(&quot;OOB_&quot;):\n            oob_tasks.append(task)\n        else:\n            print(f&quot;Unknown task: {task}&quot;)\n            \n        time.sleep(0.2)\n\ndef execute_oob_tasks_with_delay():\n    &quot;&quot;&quot;\n    Executes the saved OOB tasks after a delay.\n    &quot;&quot;&quot;\n    time.sleep(0.1)  # Short delay to ensure the LLM&#x27;s response has completed\n    execute_oob_tasks()\n\ndef execute_oob_tasks():\n    &quot;&quot;&quot;\n    Executes the saved OOB tasks.\n    &quot;&quot;&quot;\n    with open(oob_tasks_file, &#x27;r&#x27;) as f:\n        oob_tasks = json.load(f)\n\n    for task in oob_tasks:\n        task = task.strip()\n        if task.startswith(&quot;OOB_MouseMover =&quot;):\n            image_name = task.split(&quot;=&quot;)[1].strip()\n            move_mouse_to_image(image_name)\n        elif task.startswith(&quot;OOB_MouseClick =&quot;):\n            value = task.split(&quot;=&quot;)[1].strip()\n            if &quot;,&quot; in value:  # Check if the value contains coordinates\n                coordinates = value\n                left_click_coordinates_raw(coordinates)\n            else:  # Otherwise, treat it as an image name\n                image_name = value\n                left_click_image(image_name)\n        elif task.startswith(&quot;OOB_TextInput =&quot;):\n            text = task.split(&quot;=&quot;)[1].strip()\n            text_input(text)\n        elif task.startswith(&quot;OOB_SpecialKey =&quot;):\n            key = task.split(&quot;=&quot;)[1].strip()\n            special_key(key)\n        elif task.startswith(&quot;OOB_FunctionKey =&quot;):\n            key = task.split(&quot;=&quot;)[1].strip()\n            function_key(key)\n        elif task.startswith(&quot;OOB_KeyCombo =&quot;):\n            combo = task.split(&quot;=&quot;)[1].strip()\n            key_combo(combo)\n        elif task.startswith(&quot;OOB_Delay =&quot;):\n            delay = int(task.split(&quot;=&quot;)[1].strip())\n            delay_task(delay)\n        elif task.startswith(&quot;OOB_MouseMove =&quot;):\n            coordinates = task.split(&quot;=&quot;)[1].strip()\n            move_mouse_to_coordinates_raw(coordinates)\n        elif task.startswith(&quot;OOB_MouseClick =&quot;):\n            coordinates = task.split(&quot;=&quot;)[1].strip()\n            left_click_coordinates_raw(coordinates)\n        elif task == &quot;OOB_TakeScreenshot&quot;:\n            take_screenshot_task()\n        elif task == &quot;OOB_PageUp&quot;:\n            pyautogui.press(&quot;pageup&quot;)\n        elif task == &quot;OOB_PageDown&quot;:\n            pyautogui.press(&quot;pagedown&quot;)\n        else:\n            print(f&quot;Unknown OOB task: {task}&quot;)\n            \n        time.sleep(0.2)\n\ndef delay_task(delay):\n    &quot;&quot;&quot;\n    Adds a delay in milliseconds.\n    &quot;&quot;&quot;\n    time.sleep(delay / 1000.0)\n\ndef move_mouse_to_coordinates_raw(coordinates):\n    &quot;&quot;&quot;\n    Moves the mouse to the specified raw coordinates on the specified monitor.\n\n    Args:\n        coordinates (str): A string containing the monitor index and coordinates in the format &quot;monitor_index,x,y&quot;.\n    &quot;&quot;&quot;\n    monitor_index, x, y = map(int, coordinates.split(&quot;,&quot;))\n    monitors = screeninfo.get_monitors()\n\n    if monitor_index &gt;= len(monitors):\n        raise ValueError(f&quot;Monitor index {monitor_index} is out of range. There are only {len(monitors)} monitors.&quot;)\n\n    monitor = monitors[monitor_index]\n    monitor_x, monitor_y = monitor.x, monitor.y\n    absolute_x = monitor_x + x\n    absolute_y = monitor_y + y\n    pyautogui.moveTo(absolute_x, absolute_y)\n    print(f&quot;Mouse moved to coordinates: ({absolute_x}, {absolute_y}) on monitor {monitor_index}&quot;)\n\ndef left_click_coordinates_raw(coordinates):\n    &quot;&quot;&quot;\n    Moves the mouse to the specified raw coordinates on the specified monitor and performs a left click.\n\n    Args:\n        coordinates (str): A string containing the monitor index and coordinates in the format &quot;monitor_index,x,y&quot;.\n    &quot;&quot;&quot;\n    monitor_index, x, y = map(int, coordinates.split(&quot;,&quot;))\n    monitors = screeninfo.get_monitors()\n\n    if monitor_index &gt;= len(monitors):\n        raise ValueError(f&quot;Monitor index {monitor_index} is out of range. There are only {len(monitors)} monitors.&quot;)\n\n    monitor = monitors[monitor_index]\n    monitor_x, monitor_y = monitor.x, monitor.y\n    absolute_x = monitor_x + x\n    absolute_y = monitor_y + y\n    pyautogui.moveTo(absolute_x, absolute_y)\n    pyautogui.click()\n    print(f&quot;Mouse clicked at coordinates: ({absolute_x}, {absolute_y}) on monitor {monitor_index}&quot;)\n\ndef take_screenshot_task():\n    &quot;&quot;&quot;\n    Takes a screenshot using the current values from the Gradio interface.\n    &quot;&quot;&quot;\n    take_screenshot(global_vars[&quot;global_monitor_index&quot;], global_vars[&quot;global_text_queries&quot;], global_vars[&quot;global_score_threshold&quot;], global_vars[&quot;global_vision_model_question&quot;], global_vars[&quot;global_full_image_vision_model_question&quot;])\n\ndef move_mouse_to_image(image_name):\n    &quot;&quot;&quot;\n    Moves the mouse to the center of the box that defines the image.\n    &quot;&quot;&quot;\n    results_json_path = &quot;extensions/Lucid_Autonomy/ImageOutputTest/results.json&quot;\n    with open(results_json_path, &#x27;r&#x27;) as f:\n        results = json.load(f)\n\n    for result in results:\n        if result[&quot;image_name&quot;] == image_name:\n            coordinates = result[&quot;coordinates&quot;]\n            monitor = get_monitor_info(MONITOR_INDEX)\n            move_mouse_to_coordinates(monitor, coordinates)\n            break\n    else:\n        print(f&quot;Image name &#x27;{image_name}&#x27; not found in the results.&quot;)\n\ndef left_click_image(image_name):\n    &quot;&quot;&quot;\n    Moves the mouse to the center of the box that defines the image and performs a left click.\n    &quot;&quot;&quot;\n    results_json_path = &quot;extensions/Lucid_Autonomy/ImageOutputTest/results.json&quot;\n    with open(results_json_path, &#x27;r&#x27;) as f:\n        results = json.load(f)\n\n    for result in results:\n        if result[&quot;image_name&quot;] == image_name:\n            coordinates = result[&quot;coordinates&quot;]\n            monitor = get_monitor_info(MONITOR_INDEX)\n            move_mouse_to_coordinates(monitor, coordinates)\n            pyautogui.click()  # Perform a left click\n            break\n    else:\n        print(f&quot;Image name &#x27;{image_name}&#x27; not found in the results.&quot;)\n\ndef move_mouse_to_coordinates(monitor, coordinates):\n    &quot;&quot;&quot;\n    Moves the mouse cursor to the specified coordinates on the monitor.\n\n    Args:\n        monitor (dict): A dictionary containing the monitor&#x27;s information.\n        coordinates (list): A list containing the coordinates [x1, y1, x2, y2].\n    &quot;&quot;&quot;\n    x1, y1, x2, y2 = coordinates\n    monitor_x, monitor_y = monitor.x, monitor.y\n    center_x = monitor_x + (x1 + x2) // 2\n    center_y = monitor_y + (y1 + y2) // 2\n    pyautogui.moveTo(center_x, center_y)\n    print(f&quot;Mouse moved to coordinates: ({center_x}, {center_y})&quot;)\n\ndef get_monitor_info(monitor_index):\n    &quot;&quot;&quot;\n    Gets information about the specified monitor.\n\n    Args:\n        monitor_index (int): The index of the monitor to get information about.\n\n    Returns:\n        dict: A dictionary containing the monitor&#x27;s information.\n    &quot;&quot;&quot;\n    monitors = screeninfo.get_monitors()\n\n    if monitor_index &gt;= len(monitors):\n        raise ValueError(f&quot;Monitor index {monitor_index} is out of range. There are only {len(monitors)} monitors.&quot;)\n\n    return monitors[monitor_index]\n\ndef history_modifier(history):\n    &quot;&quot;&quot;\n    Modifies the chat history before the text generation in chat mode begins.\n    &quot;&quot;&quot;\n    # Clean the internal history\n    history[&quot;internal&quot;] = clean_chat_history(history[&quot;internal&quot;])\n    # Clean the visible history\n    history[&quot;visible&quot;] = clean_chat_history(history[&quot;visible&quot;])\n\n    # Inject the latest compressedresults.json into the internal history\n    compressed_results_json_path = &quot;extensions/Lucid_Autonomy/ImageOutputTest/compressedresults.json&quot;\n    with open(compressed_results_json_path, &#x27;r&#x27;) as f:\n        compressed_results = json.load(f)\n    latest_compressed_results = json.dumps(compressed_results, indent=4)\n    history[&quot;internal&quot;].append([&quot;&quot;, f&quot;&lt;START_COMPRESSED_RESULTS&gt;\\n{latest_compressed_results}\\n&lt;END_COMPRESSED_RESULTS&gt;&quot;])\n\n    return history\n\ndef clean_chat_history(history):\n    &quot;&quot;&quot;\n    Cleans the chat history by removing all but the latest instance of the contents between the bookends.\n    &quot;&quot;&quot;\n    START_BOOKEND = &quot;&lt;START_COMPRESSED_RESULTS&gt;&quot;\n    END_BOOKEND = &quot;&lt;END_COMPRESSED_RESULTS&gt;&quot;\n\n    # Find all instances of the bookends\n    bookend_indices = []\n    for i, entry in enumerate(history):\n        if isinstance(entry, list) and len(entry) == 2:\n            if START_BOOKEND in entry[1] and END_BOOKEND in entry[1]:\n                bookend_indices.append(i)\n\n    # If there are multiple instances, remove all but the last one\n    if len(bookend_indices) &gt; 1:\n        for i in bookend_indices[:-1]:\n            history[i][1] = re.sub(f&quot;{START_BOOKEND}.*?{END_BOOKEND}&quot;, &quot;&quot;, history[i][1], flags=re.DOTALL)\n\n    return history\n\ndef append_compressed_results(output, compressed_results):\n    &quot;&quot;&quot;\n    Appends the compressed results to the output.\n    &quot;&quot;&quot;\n    START_BOOKEND = &quot;&lt;START_COMPRESSED_RESULTS&gt;&quot;\n    END_BOOKEND = &quot;&lt;END_COMPRESSED_RESULTS&gt;&quot;\n    output += f&quot;\\n{START_BOOKEND}\\n{json.dumps(compressed_results, indent=4)}\\n{END_BOOKEND}&quot;\n    return output\n\ndef text_input(text):\n    &quot;&quot;&quot;\n    Types the given text, replacing &#x27;/n&#x27; with newline characters.\n    &quot;&quot;&quot;\n    text = text.replace(&quot;/n&quot;, &quot;\\n&quot;)\n    pyautogui.typewrite(text)\n\ndef special_key(key):\n    &quot;&quot;&quot;\n    Presses the specified special key.\n    &quot;&quot;&quot;\n    if key == &quot;enter&quot;:\n        pyautogui.press(&quot;enter&quot;)\n    elif key == &quot;space&quot;:\n        pyautogui.press(&quot;space&quot;)\n    elif key == &quot;backspace&quot;:\n        pyautogui.press(&quot;backspace&quot;)\n    elif key == &quot;tab&quot;:\n        pyautogui.press(&quot;tab&quot;)\n    elif key == &quot;escape&quot;:\n        pyautogui.press(&quot;esc&quot;)\n    elif key == &quot;shift&quot;:\n        pyautogui.press(&quot;shift&quot;)\n    elif key == &quot;ctrl&quot;:\n        pyautogui.press(&quot;ctrl&quot;)\n    elif key == &quot;alt&quot;:\n        pyautogui.press(&quot;alt&quot;)\n    else:\n        print(f&quot;Unsupported special key: {key}&quot;)\n\ndef function_key(key):\n    &quot;&quot;&quot;\n    Presses the specified function key.\n    &quot;&quot;&quot;\n    if key.startswith(&quot;f&quot;) and key[1:].isdigit():\n        pyautogui.press(key)\n    else:\n        print(f&quot;Unsupported function key: {key}&quot;)\n\ndef key_combo(combo):\n    &quot;&quot;&quot;\n    Presses the specified key combination.\n    &quot;&quot;&quot;\n    keys = combo.split(&quot;+&quot;)\n    for key in keys:\n        if key in [&quot;ctrl&quot;, &quot;alt&quot;, &quot;shift&quot;]:\n            pyautogui.keyDown(key)\n        else:\n            pyautogui.press(key)\n    for key in keys:\n        if key in [&quot;ctrl&quot;, &quot;alt&quot;, &quot;shift&quot;]:\n            pyautogui.keyUp(key)\n\nif __name__ == &quot;__main__&quot;:\n    # Launch the Gradio interface with the specified UI components\n    gr.Interface(\n        fn=ui,\n        inputs=None,\n        outputs=&quot;ui&quot;\n    ).launch()",
            "Thank you for sharing the code. Here are my notes and observations:\n\n### Notes and Observations\n\n1. **Imports and Dependencies**:\n   - The code imports various libraries such as `gradio`, `pyautogui`, `screeninfo`, `transformers`, `PIL`, `torch`, `os`, `shutil`, `re`, `json`, `time`, and `threading`.\n   - These libraries are used for creating a user interface, automating mouse and keyboard actions, processing images, and handling file operations.\n\n2. **Configurable Variables**:\n   - The code defines several configurable variables such as `MONITOR_INDEX`, `TEXT_QUERIES`, `SCORE_THRESHOLD`, `VISION_MODEL_QUESTION`, and `FULL_IMAGE_VISION_MODEL_QUESTION`.\n   - These variables allow for customization of the monitor to capture, text queries for object detection, score threshold for filtering predictions, and questions for the vision model.\n\n3. **Model Loading and Unloading**:\n   - The code includes functions for loading and unloading the `Owlv2` and `MiniCPM-Llama3` models.\n   - The models are loaded on-demand and unloaded after use to free up GPU memory.\n\n4. **Screenshot and Image Processing**:\n   - The `take_screenshot` function captures a screenshot of a specified monitor and processes the image using the `Owlv2` model to detect objects.\n   - The detected objects are cropped and processed with the `MiniCPM-Llama3` model to generate descriptions.\n   - The results are saved in JSON format and the output directory is cleared before saving new results.\n\n5. **Gradio Interface**:\n   - The `ui` function creates a custom Gradio interface with input fields for configuring the monitor index, text queries, score threshold, and vision model questions.\n   - The interface includes a button to trigger the screenshot and image processing functions.\n\n6. **Task Execution**:\n   - The `output_modifier` function modifies the LLM output before it is presented in the UI.\n   - It searches for the &quot;Autonomy_Tasks:&quot; trigger phrase in the LLM&#x27;s output and executes the tasks listed in the tasks section.\n   - The tasks include mouse movements, clicks, text input, special key presses, function key presses, key combinations, delays, and taking screenshots.\n   - The tasks are executed with a short delay to ensure the LLM&#x27;s response has completed.\n\n7. **Out-of-Band (OOB) Tasks**:\n   - The code includes functions for executing OOB tasks after a delay.\n   - OOB tasks are saved to a JSON file and executed separately from the main tasks.\n\n8. **Helper Functions**:\n   - The code includes several helper functions for moving the mouse, clicking, typing text, pressing keys, and handling delays.\n   - These functions use the `pyautogui` library to automate mouse and keyboard actions.\n\n### Contextualization\n\nThe code is a comprehensive implementation of the `Lucid_Autonomy` project, which aims to enable a Large Language Model (LLM) to interact autonomously with a computer. The code leverages various models and tools to detect objects on the screen, process images, and perform actions such as moving the mouse, clicking, typing text, and pressing keys. The Gradio interface provides a user-friendly way to configure and trigger these actions.\n\n### Next Steps\n\nPlease let me know what specific tasks or issues you would like me to focus on next. I can help with code review, debugging, optimization, or any other aspect of the project."
        ]
    ]
}